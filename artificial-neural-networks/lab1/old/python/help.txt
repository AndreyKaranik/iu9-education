# 'ReLU': (relu, relu_derivative)

В ходе работы была реализована модель однослойного персептрона для распознавания букв русского алфавита и цифр, используя различные функции активации: линейную, сигмоиду, гиперболический тангенс и ReLU. Результаты экспериментов, представленные в виде графиков зависимости функции потерь от числа эпох, показали, что функция ReLU обеспечила наилучшие результаты, быстро снижая функцию потерь и обеспечивая более эффективное обучение модели. В то же время линейная функция активации продемонстрировала худшие результаты, что связано с её неспособностью моделировать сложные зависимости. Сигмоида и гиперболический тангенс также показали приемлемые, но менее эффективные результаты из-за проблем с градиентным затуханием. Таким образом, ReLU оказалась наиболее подходящей для данной задачи, обеспечивая быстрое и эффективное обучение по сравнению с другими функциями активации.